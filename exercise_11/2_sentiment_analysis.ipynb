{"cells":[{"cell_type":"markdown","metadata":{"id":"n28Mk7dhHgIZ"},"source":["# Sentiment Analysis\n","\n","Congrats, you finished the part on the data preparation, and we can now move on to the more exciting part of using RNNs/LSTMs to process sequential data! But be careful, even if the previous notebook might seem a little bit boring, it is of great importance. We switched from images to text data in this exercise, and remember the first steps that we did in our class were also data related, and they were essential for all the following exercises. So naturally, since we switched to text data in this exercise, make sure you have a good understanding of how the data has been prepared.\n","\n","For the last I2DL exercise, we want to make use of Recurrent Neural Networks (RNNs) to process sequential data. We will stick with the same dataset we have been looking at in the previous notebook, namely the [IMDb](https://ai.stanford.edu/~amaas/data/sentiment/) sentiment analysis dataset that contains positive and negative movie reviews.\n","\n","<p class=\"aligncenter\">\n","    <img src=\"images/IMDB.jpg\" alt=\"centered image\" />\n","</p>\n","\n","Sentiment analysis is the task of predicting the sentiment of a text. In this notebook, you will train a network to process reviews from the dataset and evaluate whether it has been a positive or a negative review. Below are two examples:\n","\n","![Screenshot%20from%202021-01-25%2008-29-35.png](attachment:Screenshot%20from%202021-01-25%2008-29-35.png)\n"]},{"cell_type":"markdown","metadata":{"id":"XwQiF2K6HgIa"},"source":["## (Optional) Mount folder in Colab\n","\n","Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEjTiNJmHgIb","executionInfo":{"status":"ok","timestamp":1733221587942,"user_tz":-60,"elapsed":32020,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}},"outputId":"f52e5b79-11e4-4a86-9905-580713f97114"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","['1_text_preprocessing_and_embedding.ipynb', '2_sentiment_analysis.ipynb', 'Optional-recurrent_neural_networks.ipynb', 'exercise_code', 'images', 'models']\n"]}],"source":["# Use the following lines if you want to use Google Colab\n","# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n","# NOTE: terminate all other colab sessions that use GPU!\n","# NOTE 2: Make sure the correct exercise folder (e.g exercise_11) is given.\n","\n","# \"\"\"\n","from google.colab import drive\n","import os\n","\n","gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))\n","# \"\"\""]},{"cell_type":"markdown","metadata":{"id":"9hqTR6i3HgIb"},"source":["### Set up PyTorch environment in colab\n","- (OPTIONAL) Enable GPU via Runtime --> Change runtime type --> GPU\n","- Uncomment the following cell if you are using the notebook in google colab:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbnrvNorHgIb","outputId":"69984a12-6667-4b18-dbb7-b7240c59280a","executionInfo":{"status":"ok","timestamp":1733221731029,"user_tz":-60,"elapsed":143090,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.11.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1637.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m772.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.12.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (22.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu113) (4.12.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (11.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2024.8.30)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu121\n","    Uninstalling torch-2.5.1+cu121:\n","      Successfully uninstalled torch-2.5.1+cu121\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.20.1+cu121\n","    Uninstalling torchvision-0.20.1+cu121:\n","      Successfully uninstalled torchvision-0.20.1+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","peft 0.13.2 requires torch>=1.13.0, but you have torch 1.11.0+cu113 which is incompatible.\n","torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.11.0+cu113 torchvision-0.12.0+cu113\n","Collecting tensorboard==2.8.0\n","  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.68.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (2.27.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.8.0)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (3.7)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (1.26.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (4.25.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (2.32.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (75.1.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard==2.8.0)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.8.0)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (3.1.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.8.0) (0.45.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.8.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.8.0) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard==2.8.0) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.8.0) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.8.0) (3.2.2)\n","Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, tensorboard\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.2\n","    Uninstalling tensorboard-data-server-0.7.2:\n","      Successfully uninstalled tensorboard-data-server-0.7.2\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.1\n","    Uninstalling google-auth-oauthlib-1.2.1:\n","      Successfully uninstalled google-auth-oauthlib-1.2.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.1\n","    Uninstalling tensorboard-2.17.1:\n","      Successfully uninstalled tensorboard-2.17.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pandas-gbq 0.24.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n","tensorflow 2.17.1 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1\n","Collecting pytorch-lightning==1.6.0\n","  Downloading pytorch_lightning-1.6.0-py3-none-any.whl.metadata (33 kB)\n","\u001b[33mWARNING: Ignoring version 1.6.0 of pytorch-lightning since it has invalid metadata:\n","Requested pytorch-lightning==1.6.0 from https://files.pythonhosted.org/packages/09/18/cee67f4849dea9a29b7af7cdf582246bcba9eaa73d9443e138a4172ec786/pytorch_lightning-1.6.0-py3-none-any.whl has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n","    torch (>=1.8.*)\n","           ~~~~~~^\n","Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-lightning==1.6.0 (from versions: 0.0.2, 0.2, 0.2.2, 0.2.3, 0.2.4, 0.2.4.1, 0.2.5, 0.2.5.1, 0.2.5.2, 0.2.6, 0.3, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.4.1, 0.3.5, 0.3.6, 0.3.6.1, 0.3.6.3, 0.3.6.4, 0.3.6.5, 0.3.6.6, 0.3.6.7, 0.3.6.8, 0.3.6.9, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.4.8, 0.4.9, 0.5.0, 0.5.1, 0.5.1.2, 0.5.1.3, 0.5.2, 0.5.2.1, 0.5.3, 0.5.3.1, 0.5.3.2, 0.5.3.3, 0.6.0, 0.7.1, 0.7.3, 0.7.5, 0.7.6, 0.8.1, 0.8.3, 0.8.4, 0.8.5, 0.9.0, 0.10.0, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.2.6, 1.2.7, 1.2.8, 1.2.9, 1.2.10, 1.3.0rc1, 1.3.0rc2, 1.3.0rc3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.3.7.post0, 1.3.8, 1.4.0rc0, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.4.6, 1.4.7, 1.4.8, 1.4.9, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.5.5, 1.5.6, 1.5.7, 1.5.8, 1.5.9, 1.5.10, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.7.4, 1.7.5, 1.7.6, 1.7.7, 1.8.0rc0, 1.8.0rc1, 1.8.0rc2, 1.8.0, 1.8.0.post1, 1.8.1, 1.8.2, 1.8.3, 1.8.3.post0, 1.8.3.post1, 1.8.3.post2, 1.8.4, 1.8.4.post0, 1.8.5, 1.8.5.post0, 1.8.6, 1.9.0rc0, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.9.4, 1.9.5, 2.0.0rc0, 2.0.0, 2.0.1, 2.0.1.post0, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.0.9.post0, 2.1.0rc0, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0rc0, 2.2.0, 2.2.0.post0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.4.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-lightning==1.6.0\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# Optional: install correct libraries in google colab\n","!python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n","!python -m pip install tensorboard==2.8.0\n","!python -m pip install pytorch-lightning==1.6.0"]},{"cell_type":"markdown","metadata":{"id":"wPpVhKixHgIb"},"source":["# 0. Setup\n","\n","As always, we first import some packages to setup the notebook."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"OSxlYhAuHgIc","executionInfo":{"status":"ok","timestamp":1733221737441,"user_tz":-60,"elapsed":6419,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"outputs":[],"source":["import os\n","import random\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.nn.utils import clip_grad_norm_\n","\n","from exercise_code.rnn.sentiment_dataset import (\n","    download_data,\n","    load_sentiment_data,\n","    load_vocab,\n","    SentimentDataset,\n","    collate\n",")\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"MkEkqRYaHgIc"},"source":["# 1. Loading Data\n","\n","As we have learned from the notebook 1, this time we not only load the raw data, but also have the corresponding vocabulary. Let us load the data that we prepared for you:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"JOG02cXtHgIc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733221749811,"user_tz":-60,"elapsed":12380,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}},"outputId":"447df5d1-3d02-4159-b691-364e09157f80"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of training samples: 9154\n","number of validation samples: 3133\n","number of test samples: 3083\n"]}],"source":["i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n","data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"SentimentData\")\n","base_dir = download_data(data_root)\n","vocab = load_vocab(base_dir)\n","train_data, val_data, test_data = load_sentiment_data(base_dir, vocab)\n","\n","print(\"number of training samples: {}\".format(len(train_data)))\n","print(\"number of validation samples: {}\".format(len(val_data)))\n","print(\"number of test samples: {}\".format(len(test_data)))"]},{"cell_type":"markdown","metadata":{"id":"N6A65ox2HgId"},"source":["## Dataset Samples\n","\n","Our raw data consists of tuples `(raw_text, token_list, token_indices, label)`. Let's sample some relatively short texts from our dataset to have a sense how it looks like:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"vKjsOBKxHgId","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733221749812,"user_tz":-60,"elapsed":28,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}},"outputId":"bde3bf70-7d25-4cef-d537-d739fb567d2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Text: \n"," Ming The Merciless does a little Bardwork and a movie most foul!\n","\n","Tokens: \n"," ['ming', 'the', 'merciless', 'does', 'a', 'little', 'bardwork', 'and', 'a', 'movie', 'most', 'foul']\n","\n","Indices: \n"," [1, 2, 1, 142, 3, 121, 1, 4, 3, 13, 99, 2850]\n","\n","Label:\n"," 0\n","\n","\n","Text: \n"," Long, boring, blasphemous. Never have I been so glad to see ending credits roll.\n","\n","Tokens: \n"," ['long', 'boring', 'blasphemous', 'never', 'have', 'i', 'been', 'so', 'glad', 'to', 'see', 'ending', 'credits', 'roll']\n","\n","Indices: \n"," [209, 238, 1, 115, 26, 7, 90, 34, 801, 6, 55, 256, 850, 1403]\n","\n","Label:\n"," 0\n","\n","\n","Text: \n"," A rating of \"1\" does not begin to express how dull, depressing and relentlessly bad this movie is.\n","\n","Tokens: \n"," ['a', 'rating', 'of', '1', 'does', 'not', 'begin', 'to', 'express', 'how', 'dull', 'depressing', 'and', 'relentlessly', 'bad', 'this', 'movie', 'is']\n","\n","Indices: \n"," [3, 512, 5, 241, 142, 24, 1095, 6, 2747, 83, 552, 2227, 4, 1, 59, 10, 13, 9]\n","\n","Label:\n"," 0\n","\n","\n","Text: \n"," Comment this movie is impossible. Is terrible, very improbable, bad interpretation e direction. Not look!!!!!\n","\n","Tokens: \n"," ['comment', 'this', 'movie', 'is', 'impossible', 'is', 'terrible', 'very', 'improbable', 'bad', 'interpretation', 'e', 'direction', 'not', 'look']\n","\n","Indices: \n"," [626, 10, 13, 9, 1216, 9, 270, 41, 4724, 59, 2664, 1010, 397, 24, 164]\n","\n","Label:\n"," 0\n","\n","\n","Text: \n"," Smallville episode Justice is the best episode of Smallville ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! It's my favorite episode of Smallville! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n","\n","Tokens: \n"," ['smallville', 'episode', 'justice', 'is', 'the', 'best', 'episode', 'of', 'smallville', 'it', 's', 'my', 'favorite', 'episode', 'of', 'smallville']\n","\n","Indices: \n"," [1, 340, 1308, 9, 2, 91, 340, 5, 1, 8, 16, 47, 352, 340, 5, 1]\n","\n","Label:\n"," 1\n","\n","\n","Text: \n"," I don't know why I like this movie so well, but I never get tired of watching it.\n","\n","Tokens: \n"," ['i', 'don', 't', 'know', 'why', 'i', 'like', 'this', 'movie', 'so', 'well', 'but', 'i', 'never', 'get', 'tired', 'of', 'watching', 'it']\n","\n","Indices: \n"," [7, 74, 23, 126, 138, 7, 32, 10, 13, 34, 68, 17, 7, 115, 82, 1225, 5, 116, 8]\n","\n","Label:\n"," 1\n","\n","\n"]}],"source":["sample_data0 = [datum for datum in train_data if len(datum[1]) < 20 and datum[-1] == 0] # negative\n","sample_data1 = [datum for datum in train_data if len(datum[1]) < 20 and datum[-1] == 1] # positive\n","\n","# we sample 2 tuples each from positive set and negative set\n","sample_data = random.sample(sample_data0, 4) + random.sample(sample_data1, 2)\n","for text, tokens, indices, label in sample_data:\n","    print('Text: \\n {}\\n'.format(text))\n","    print('Tokens: \\n {}\\n'.format(tokens))\n","    print('Indices: \\n {}\\n'.format(indices))\n","    print('Label:\\n {}\\n'.format(label))\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"D2qHZk84HgId"},"source":["## Checking the Vocabulary\n","\n","In the previous notebook, we discussed the need of a vocabulary for mapping words to unique integer IDs. Instead of creating the vocabulary manually, we provide you with the vocabulary. Let's have a look at some samples from the vocabulary of the dataset:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"bTdxdRbHHgId","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733221749812,"user_tz":-60,"elapsed":23,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}},"outputId":"6a8d72fa-a892-4cf2-a147-0ca27d742c5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 5002 \n","\n","  Sample words\n","--------------------\n"," uplifting\n"," mix\n"," forward\n"," recognized\n"," text\n"," italy\n"," concerning\n"," streisand\n"," suggested\n"," being\n"]}],"source":["print('Vocabulary size:', len(vocab), '\\n\\n  Sample words\\n{}'.format('-' * 20))\n","sample_words = random.sample(list(vocab.keys()), 10)\n","for word in sample_words:\n","    print(' {}'.format(word))"]},{"cell_type":"markdown","metadata":{"id":"wE0DvP8-HgId"},"source":["Also we saw that there are already indices in the raw data that we loaded. We can check if the indices in the vocabulary match the raw data using the last sentence in `sample_data`. Words that are not in the vocabulary are assigned to the symbol `<unk>`. The output of the following cell should be the same as the indices in the last example of our loaded raw data:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"tkCwMBKaHgIe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733221749812,"user_tz":-60,"elapsed":15,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}},"outputId":"994bb779-ef21-4d27-96d2-12f18ebe7386"},"outputs":[{"output_type":"stream","name":"stdout","text":["Text: \n"," I don't know why I like this movie so well, but I never get tired of watching it.\n","\n","Tokens: \n"," ['i', 'don', 't', 'know', 'why', 'i', 'like', 'this', 'movie', 'so', 'well', 'but', 'i', 'never', 'get', 'tired', 'of', 'watching', 'it']\n","\n","Indices: \n"," [7, 74, 23, 126, 138, 7, 32, 10, 13, 34, 68, 17, 7, 115, 82, 1225, 5, 116, 8]\n","\n","Label:\n"," 1\n","\n","Indices drawn from vocabulary: \n"," [7, 74, 23, 126, 138, 7, 32, 10, 13, 34, 68, 17, 7, 115, 82, 1225, 5, 116, 8]\n","\n"]}],"source":["# Last sample from above\n","(text, tokens, indices, label) = sample_data[-1]\n","print('Text: \\n {}\\n'.format(text))\n","print('Tokens: \\n {}\\n'.format(tokens))\n","print('Indices: \\n {}\\n'.format(indices))\n","print('Label:\\n {}\\n'.format(label))\n","\n","# Compare with the vocabulary\n","print('Indices drawn from vocabulary: \\n {}\\n'.format([vocab.get(x, vocab['<unk>']) for x in sample_data[-1][1]]))"]},{"cell_type":"markdown","metadata":{"id":"kFsGKtDhHgIe"},"source":["## Wrapping to PyTorch Datasets\n","\n","Great, the raw data is loaded properly and the vocabulary is matching. Let us wrap our data in a PyTorch dataset. For more details, check out the previous notebook and the corresponding dataset class defined in `exercise_code/rnn/sentiment_dataset.py`."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-S1-Fk1LHgIe","executionInfo":{"status":"ok","timestamp":1733221749813,"user_tz":-60,"elapsed":11,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"outputs":[],"source":["# Define a Dataset Class for train, val and test set\n","train_dataset = SentimentDataset(train_data)\n","val_dataset = SentimentDataset(val_data)\n","test_dataset = SentimentDataset(test_data)"]},{"cell_type":"markdown","metadata":{"id":"wSDnxEBMHgIe"},"source":["# 2. Creating a Sentiment Classifier\n","\n","After we have loaded the data, it is time to define a model and start training and testing."]},{"cell_type":"markdown","metadata":{"id":"MACj3lQQHgIe"},"source":["## Evaluation Metrics\n","\n","Since we just need to predict positive or negative, we can use `binary cross-entropy loss` to train our model. And accuracy can be used to assess our model's performance. We will use the following evaluation model to compute the accuracy."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"wNfY5_1IHgIe","executionInfo":{"status":"ok","timestamp":1733221749813,"user_tz":-60,"elapsed":10,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"outputs":[],"source":["bce_loss = nn.BCELoss()\n","test_loss_history = []\n","@torch.no_grad()\n","def compute_accuracy(model, data_loader, test_loss_history):\n","    corrects = 0\n","    total = 0\n","    device = next(model.parameters()).device\n","\n","    bce_loss = nn.BCELoss()\n","\n","\n","    for i, x in enumerate(data_loader, test_loss_history):\n","        input = x['data'].to(device)\n","        lengths = x['lengths']\n","        label = x['label'].to(device)\n","        pred = model(input, lengths)\n","\n","        loss = bce_loss(pred, label)\n","        test_loss_history += loss.item()\n","\n","        corrects += ((pred > 0.5) == label).sum().item()\n","\n","        total += label.numel()\n","\n","        if i > 0  and i % 100 == 0:\n","            print('Step {} / {}'.format(i, len(data_loader)))\n","\n","    return corrects / total, test_loss_history"]},{"cell_type":"code","source":[],"metadata":{"id":"88vi99rEvdXL","executionInfo":{"status":"ok","timestamp":1733221749813,"user_tz":-60,"elapsed":9,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOgr2YIjHgIe"},"source":["## Step 1: Design your own model\n","\n","In this part, you need to create a classifier using the Embedding layers you implemented in the first notebook and LSTM. For the LSTM, you may also use the PyTorch implementation.\n","\n","![Screenshot%20from%202021-01-25%2008-43-02.png](attachment:Screenshot%20from%202021-01-25%2008-43-02.png)\n","\n","\n"]},{"cell_type":"markdown","source":["\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement a Classifier</h3>\n","    \n","   Go to <code>exercise_code/rnn/text_classifiers.py</code> and implement the <code>RNNClassifier</code>. In the skeleton code, we inherited <code>nn.Module</code>. You can also inherit <code>LightningModule</code> if you want to use PyTorch Lightning.\n","</div>\n","\n","This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically).\n","The only rules your model design has to follow are:\n","* Inherit from `torch.nn.Module` or `pytorch_lightning.LightningModule`\n","* Perform the forward pass in `forward()`.\n","* Have less than 2 million parameters\n","* Have a model size of less than 50MB after saving\n","\n","After you finished, edit the below cell to make sure your implementation is correct. You should define the model yourself, which should be small enough (2 Mio. parameters) and have correct output format.\n"],"metadata":{"id":"XrYSzjUxIY0K"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"h22ML1XnHgIf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733221753136,"user_tz":-60,"elapsed":3332,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}},"outputId":"fdbec5e9-fe8c-4eac-d94d-c340749ea35c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of parameters: 565749\n","Your model is sufficiently small :)\n","All output tests are passed :)!\n"]},{"output_type":"execute_result","data":{"text/plain":["(True, True)"]},"metadata":{},"execution_count":10}],"source":["from exercise_code.rnn.tests import classifier_test, parameter_test\n","from exercise_code.rnn.text_classifiers import RNNClassifier\n","\n","########################################################################\n","# TODO - Create a Model                                               #\n","########################################################################\n","\n","model = RNNClassifier(num_embeddings = len(vocab), embedding_dim = 50, hidden_size = 256)\n","# pl_model = pl_RNNClassifier(num_embeddings = len(vocab), embedding_dim = 16, hidden_size = 512 )\n","\n","\n","########################################################################\n","#                           END OF YOUR CODE                           #\n","########################################################################\n","\n","# Check whether your model is sufficiently small and have a correct output format\n","parameter_test(model), classifier_test(model, len(vocab))"]},{"cell_type":"markdown","metadata":{"id":"YI2yzmltHgIf"},"source":["## Step 2: Train your own model\n","\n","In this section, you need to train the classifier you created. Below, you can see some setup code we provided to you. Note the **collate function** used with the `DataLoader`. If you forgot why we need the collate function here, check this out in Notebook 1.\n","\n","You are free to change the below configs (e.g. batch size, device setting etc.) as you wish."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"nUzZDmaAHgIf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733221758276,"user_tz":-60,"elapsed":5149,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}},"outputId":"cd1ab83b-67c0-4c01-ff98-ff96dffa3896"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda...\n","\n"]}],"source":["# Training configs\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","print('Using {}...\\n'.format(device))\n","\n","# Move model to the device we are using\n","model = model.to(device)\n","\n","# To tackle with the exploding gradient problem, you may want to set gclip and use clip_grad_norm_\n","# see the optional notebook for the explanation\n","gclip = None\n","\n","# data set\n","train_dataset = SentimentDataset(train_data)\n","val_dataset = SentimentDataset(val_data)\n","test_dataset = SentimentDataset(test_data)\n","\n","\n","# Dataloaders, note the collate function\n","train_loader = DataLoader(\n","  train_dataset, batch_size=16, collate_fn=collate, drop_last=True\n",")\n","val_loader = DataLoader(\n","  val_dataset, batch_size=16, collate_fn=collate, drop_last=False\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"xtgyvqigHgIf"},"source":["<div class=\"alert alert-info\">\n","<h3>Task: Implement Training</h3>\n","    <p>\n","        In the below cell, you are expected to implement your training loop to train your model. You can use the training loader provided above for iterating over the data. If you want to evaluate your model periodically, you may use the validation loader provided above. You can use pure PyTorch or PyTorch Lightning.\n","   </p>\n","</div>\n","\n","**Hints :**\n","* Use `torch.nn.BCELoss` as loss function\n","* Revise the previous exercises if you get stuck.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"cs7xneJzHgIf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d0d13b55-60b7-4a5f-cff7-184dcfb16821","executionInfo":{"status":"error","timestamp":1733221759338,"user_tz":-60,"elapsed":1071,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]},{"output_type":"error","ename":"TypeError","evalue":"Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-3fdc7e59a63f>\u001b[0m in \u001b[0;36m<cell line: 127>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# Tensorboard, summarywriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/MyDrive/i2dl/exercise_10'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummaryWriter\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecordWriter\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSessionLog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/event_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_summary__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mcontaining_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   fields=[\n\u001b[0;32m---> 36\u001b[0;31m     _descriptor.FieldDescriptor(\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tensorboard.TensorShapeProto.Dim.size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpp_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mhas_default_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining_oneof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001b[0;32m--> 553\u001b[0;31m       \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFindExtensionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"]}],"source":["########################################################################\n","#                     TODO - Train Your Model                          #\n","########################################################################\n","\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n","import numpy as np\n","\n","hparams = {\n","    \"epochs\" : 50,\n","\n","    # optimizer\n","    \"learning_rate\" : 1e-4,\n","    \"weight_decay\": 1e-9,\n","    'momentum': 0.9,\n","\n","    # clipping\n","    'gclip' : 10\n","}\n","\n","\n","# moveing the model to the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model = model.to(device)\n","\n","# initializing thel ist for storing the loss and accuracy\n","train_loss_history = []\n","val_loss_history = []\n","lr_history = []\n","\n","# get the number of batches\n","def get_num_batches(dataloader, data):\n","  return int(len(batch) // dataloader.batch_size)\n","\n","# TODO : Learning Scheduler : https://velog.io/@idj7183/Competition%EC%97%90%EC%84%9C-%EC%83%88%EB%A1%9C-%EB%B0%B0%EC%9A%B4-%EA%B2%83\n","def get_scheduler(optimizer, step_size, gamma):\n","  return torch.optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)\n","\n","# create progress bar\n","def create_tqdm_bar(iterable, desc):\n","  return tqdm(enumerate(iterable), total=len(iterable), ncols = 150, desc = desc, leave = False)\n","\n","# Training\n","def train_model(model, train_loader, val_loader, loss_func, optim, writer, gclip=30, epochs = 50):\n","  optimizer = optim\n","  # scheduler = get_scheduler(optimizer = optimizer, step_size = epochs * len(train_loader) / 20, gamma = 0.6)\n","  # scheduler = get_scheduler(optimizer = optimizer, step_size = 50, gamma = 0.7)\n","  scheduler = ReduceLROnPlateau(optimizer, 'min', factor = 0.6, min_lr = 1e-8)\n","  # scheduler = CosineAnnealingWarmRestarts(optimizer,\n","                                        # T_0 = 8,# Number of iterations for the first restart\n","                                        # T_mult = 1, # A factor increases TiTi​ after a restart\n","                                        # eta_min = 1e-4) # Minimum learning rate\n","\n","  for epoch in range(epochs):\n","    # train\n","    model.train()\n","    train_running_loss = 0  # 출력용\n","    for train_iteration, train_batch in enumerate(train_loader):\n","      #\n","      seq = train_batch['data'].to(device)\n","      label = train_batch['label'].to(device)\n","      seq_lens = train_batch['lengths']\n","\n","      optimizer.zero_grad()\n","      # print('seq : ', seq)\n","      # print('seq len : ', seq_lens)\n","\n","      pred = model(seq, seq_lens)\n","      loss = loss_func(pred, label)\n","      train_running_loss += loss.item()\n","\n","\n","\n","      loss.backward()\n","\n","      # clipping the gradient\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n","\n","      optimizer.step()\n","\n","\n","      # if ((train_iteration + 1) % 100) == 0:\n","      # print('-------step {}/{}, train loss : {}'.format(train_iteration + 1, len(train_loader), loss.item()))\n","\n","    # update step based scheduler\n","    # scheduler.step()\n","\n","    # validation\n","    model.eval()\n","    validation_running_loss = 0 # 출력용\n","    with torch.no_grad():\n","      # for val_iteration, val_batch in validation_loop:\n","      for val_iteration, val_batch in enumerate(val_loader):\n","        seq = val_batch['data'].to(device)\n","        label = val_batch['label'].to(device)\n","        seq_lens = val_batch['lengths']\n","\n","        pred = model(seq, seq_lens)\n","        loss = loss_func(pred, label)\n","\n","        # validation_loss += loss.item()\n","        validation_running_loss += loss.item()\n","\n","\n","\n","\n","        # if ((val_iteration + 1) % 100) == 0:\n","          # print('-------step {}/{}, validation loss : {}'.format(val_iteration + 1, len(val_loader), loss.item()))\n","\n","    # save loss for plotting\n","    train_loss_history.append(train_running_loss / len(train_loader))\n","    val_loss_history.append(validation_running_loss / len(val_loader))\n","    lr_history.append(optimizer.param_groups[0]['lr'])\n","\n","\n","    # learning rate step when plateau <- Plateau Scheduler\n","    scheduler.step(validation_running_loss / len(val_loader))\n","\n","    # print current lr\n","    print('current learning rate : ',  scheduler._last_lr)\n","\n","    # print current validation loss\n","    print(\"EPOCH %04d / %04d | %12s : %.8f | %12s : %0.8f\"  % (epoch + 1, epochs, 'VALIDATION LOSS', validation_running_loss / len(val_loader) , 'TRAIN LOSS', train_running_loss / len(train_loader)))\n","\n","# Tensorboard, summarywriter\n","from torch.utils.tensorboard import SummaryWriter\n","base_dir = '/content/gdrive/MyDrive/i2dl/exercise_10'\n","log_dir = os.path.join(base_dir, 'log')\n","writer = SummaryWriter(log_dir = log_dir)\n","\n","\n","# optimizer and loss function\n","optimizer = optim.Adam(model.parameters(),\n","                       lr = hparams['learning_rate'],\n","                       weight_decay = hparams['weight_decay'])\n","\n","loss_func = torch.nn.BCELoss()\n","\n","# train\n","epochs = hparams['epochs']\n","gclip = hparams['gclip']\n","train_model(model, train_loader, val_loader, loss_func, optimizer, writer, gclip, epochs)\n","\n","# save the loss on tensorboard\n","writer.close()\n","\n","\n","########################################################################\n","#                           END OF YOUR CODE                           #\n","########################################################################"]},{"cell_type":"markdown","metadata":{"id":"O1N8X54lHgIf"},"source":["## Testing the Model\n","\n","As you trained a model and improved it on the validation set, you can now test it on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvTs21lgHgIf","executionInfo":{"status":"aborted","timestamp":1733221759339,"user_tz":-60,"elapsed":19,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"outputs":[],"source":["test_loader = DataLoader(\n","  test_dataset, batch_size=8, collate_fn=collate, drop_last=False\n",")\n","\n","a, b, test_loss_history = compute_accuracy(model, test_loader, test_loss_history)\n","print(\"accuracy on test set: {}\".format())\n","\n","\n","test_loss_history = [x.cpu() for x in test_loss_history]\n","test_loss_history[0]\n","# plt.plot(test_loss_history, label = \"Test Loss\")\n","# plt.xlabel(\"Iteration\")\n","# plt.ylabel(\"Loss\")\n","# plt.legend()\n","# plt.title('Training and Validation Loss')\n","# plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"zLqZxG4KHgIg"},"source":["## Demo\n","\n","\n","Now that you trained a sufficiently good sentiment classifier, run the below cell and type some text to see some predictions (type exit to quit the demo). Since we used a small data, don't expect too much :).\n","<div class=\"alert alert-warning\">\n","<h3>Warning!</h3>\n","    <p>\n","        As there is a while True loop in the cell below, you can skip this one for now and run the cell under '3. Submission' first to save your model.\n","   </p>\n","</div>"]},{"cell_type":"code","source":["plt.plot(val_loss_history, label = \"Validation Loss\")\n","plt.plot(train_loss_history, label = \"Train Loss\")\n","plt.plot(lr_history, label = \"Learning Rate\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.title('Training and Validation Loss')\n","plt.show()"],"metadata":{"id":"gWaakLMboLXR","executionInfo":{"status":"aborted","timestamp":1733221759339,"user_tz":-60,"elapsed":18,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiZBzX-bHgIg","executionInfo":{"status":"aborted","timestamp":1733221759339,"user_tz":-60,"elapsed":16,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"outputs":[],"source":["from exercise_code.rnn.sentiment_dataset import tokenize\n","\n","text = ''\n","w2i = vocab\n","while True:\n","    text = input()\n","    if text == 'exit':\n","        break\n","\n","    words = torch.tensor([\n","        w2i.get(word, w2i['<unk>'])\n","        for word in tokenize(text)\n","    ]).long().to(device).view(-1, 1)  # T x B\n","\n","    pred = model(words).item()\n","    sent = pred > 0.5\n","\n","    print('Sentiment -> {}, Confidence -> {}'.format(\n","        ':)' if sent else ':(', pred if sent else 1 - pred\n","    ))\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"UFZ4oqQuHgIg"},"source":["fuc# 3. Submission\n","\n","If you got sufficient performance on the test data, you are ready to upload your model to the [server](https://i2dl.vc.in.tum.de/submission/) . As always, let's first save your final model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVJZz1isHgIg","executionInfo":{"status":"aborted","timestamp":1733221759339,"user_tz":-60,"elapsed":15,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"outputs":[],"source":["from exercise_code.util.save_model import save_model\n","\n","save_model(model, 'rnn_classifier.p')"]},{"cell_type":"markdown","metadata":{"id":"x4dqADoiHgIg"},"source":["Congrats, you finished the last I2DL exercise! One last time this semester, let's prepare the submission:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhQNiZ6HHgIg","executionInfo":{"status":"aborted","timestamp":1733221759339,"user_tz":-60,"elapsed":14,"user":{"displayName":"Jaeyeop Chung","userId":"07183897499487652795"}}},"outputs":[],"source":["# Now zip the folder for upload\n","from exercise_code.util.submit import submit_exercise\n","\n","submit_exercise('../output/exercise11')"]},{"cell_type":"markdown","metadata":{"id":"b3vuXYVQHgIg"},"source":["# 4. Submission Instructions\n","\n","Congratulations! You've just built your first image classifier! To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n","\n","1. Go on [our submission page](https://i2dl.vc.in.tum.de/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum-online and check your mails there. You will get an id which we need in the next step.\n","2. Log into [our submission page](https://i2dl.vc.in.tum.de/submission/) with your account details and upload the zip file.\n","3. Your submission will be evaluated by our system and you will get feedback about the performance of it. You will get an email with your score as well as a message if you have surpassed the threshold.\n","4. Within the working period, you can submit as many solutions as you want to get the best possible score.\n","\n","# 5. Submission Goals\n","\n","- Goal: Implement and train a recurrent neural network for sentiment analysis.\n","- Passing Criteria: Reach **Accuracy >= 83%** on __our__ test dataset. The submission system will show you your score after you submit.\n","\n","- Submission start: __January 26, 2023, 13.00__\n","- Submission deadline: __February 1, 2023, 15.59__\n","- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus."]},{"cell_type":"markdown","metadata":{"id":"NdlTF7w4HgIg"},"source":["# [Exercise Review](https://docs.google.com/forms/d/e/1FAIpQLSedSHEJ5vN-9FmJN-IGSQ9heDM_8qJQjHL4glgQGlrpQJEYPQ/viewform?usp=pp_url&entry.999074405=Exercise+11:+RNNs)\n","\n","We are always interested in your opinion. Now that you have finished this exercise, we would like you to give us some feedback about the time required to finish the submission and/or work through the notebooks. Please take the short time to fill out our [review form](https://docs.google.com/forms/d/e/1FAIpQLSedSHEJ5vN-9FmJN-IGSQ9heDM_8qJQjHL4glgQGlrpQJEYPQ/viewform?usp=pp_url&entry.999074405=Exercise+11:+RNNs) for this exercise so that we can do better next time! :)"]}],"metadata":{"kernelspec":{"display_name":"i2dl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"ae3aae73068e3f6c78354faadc00aa3f23e0713f86a27300232dd83e2bc002d8"}},"colab":{"provenance":[],"machine_shape":"hm","gpuClass":"premium"},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}