{"cells":[{"cell_type":"markdown","metadata":{"id":"n7G2Ut_zq5ME"},"source":["Semantic Segmentation \n","============\n","\n","In this exercise you are going to work on a computer vision task called semantic segmentation. In comparison to image classification the goal is not to classify an entire image but each of its pixels separately. This implies that the  output of the network is not a single scalar but a segmentation with the same shape as the input image. Think about why you should rather use convolutional than fully-connected layers for this task!\n","\n","\u003cimg src=\"https://camo.githubusercontent.com/d10b897e15344334e449104a824aff6c29125dc2/687474703a2f2f63616c76696e2e696e662e65642e61632e756b2f77702d636f6e74656e742f75706c6f6164732f646174612f636f636f7374756666646174617365742f636f636f73747566662d6578616d706c65732e706e67\"\u003e"]},{"cell_type":"markdown","metadata":{"id":"vuHczxQxq5MG"},"source":["## (Optional) Mount folder in Colab\n","\n","Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3723,"status":"ok","timestamp":1674621827047,"user":{"displayName":"Jay Chung","userId":"07183897499487652795"},"user_tz":-60},"id":"kNEwzzF8q5MH","outputId":"edcc29d4-cc1c-415b-8137-b6ae41d6edbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n","['.tmp', '1_segmentation_nn.ipynb', 'exercise_code', 'images', 'lightning_logs', 'log', 'logs', 'models']\n"]}],"source":["# Use the following lines if you want to use Google Colab\n","# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n","# NOTE: terminate all other colab sessions that use GPU!\n","# NOTE 2: Make sure the correct exercise folder (e.g exercise_10) is given.\n","\n","# \"\"\"\n","from google.colab import drive\n","import os\n","\n","gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_10'\n","\n","# This will mount your google drive under 'MyDrive'\n","drive.mount('/content/gdrive', force_remount=True)\n","# In order to access the files in this notebook we have to navigate to the correct folder\n","os.chdir(gdrive_path)\n","# Check manually if all files are present\n","print(sorted(os.listdir()))\n","# \"\"\""]},{"cell_type":"markdown","metadata":{"id":"6eJG4IUGq5MI"},"source":["### Set up PyTorch environment in colab\n","- (OPTIONAL) Enable GPU via Runtime Change runtime type --\u003e GPU --\u003e\n","- Uncomment the following cell if you are using the notebook in google colab:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"PUwXhvvgq5MI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.11.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl (1637.0 MB)\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m197.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1636999168 bytes == 0x3eb8000 @  0x7feced3bf680 0x7feced3e0824 0x5b3128 0x5bbc90 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m200.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2046255104 bytes == 0x657e2000 @  0x7feced3bf680 0x7feced3dfda2 0x5f714c 0x64d800 0x527022 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x5f5ee6 0x56bbe1 0x569d8a 0x5f60c3 0x56cc92 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a\n","tcmalloc: large alloc 1636999168 bytes == 0x3eb8000 @  0x7feced3bf680 0x7feced3e0824 0x5f97c1 0x5f8ecc 0x504866 0x56bbe1 0x569d8a 0x5f60c3 0x56bbe1 0x569d8a 0x5f60c3 0x50b32c 0x5f6b7b 0x66731d 0x5f6706 0x571143 0x50b22e 0x570b82 0x569d8a 0x50b3a0 0x570b82 0x569d8a 0x50b3a0 0x56cc92 0x501044 0x56be83 0x501044 0x56be83 0x501044 0x56be83 0x5f5ee6\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.12.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp38-cp38-linux_x86_64.whl (22.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.11.0+cu113) (4.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0+cu113) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0+cu113) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.12.0+cu113) (2.25.1)\n","Requirement already satisfied: chardet\u003c5,\u003e=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etorchvision==0.12.0+cu113) (4.0.0)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etorchvision==0.12.0+cu113) (2.10)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etorchvision==0.12.0+cu113) (1.24.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etorchvision==0.12.0+cu113) (2022.12.7)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.1+cu116\n","    Uninstalling torchvision-0.14.1+cu116:\n","      Successfully uninstalled torchvision-0.14.1+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.11.0+cu113 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.11.0+cu113 torchvision-0.12.0+cu113\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboard==2.9.0\n","  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (2.16.0)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (57.4.0)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (2.25.1)\n","Requirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (0.38.4)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (3.4.1)\n","Requirement already satisfied: grpcio\u003e=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (1.51.1)\n","Requirement already satisfied: numpy\u003e=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (1.21.6)\n","Requirement already satisfied: protobuf\u003e=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (3.19.6)\n","Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (1.8.1)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (0.6.1)\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.9.0) (1.3.0)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard==2.9.0) (0.2.8)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard==2.9.0) (5.2.1)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard==2.9.0) (4.9)\n","Requirement already satisfied: six\u003e=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard==2.9.0) (1.15.0)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard==2.9.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard==2.9.0) (6.0.0)\n","Requirement already satisfied: chardet\u003c5,\u003e=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard==2.9.0) (4.0.0)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard==2.9.0) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.8/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard==2.9.0) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard==2.9.0) (2022.12.7)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard==2.9.0) (3.11.0)\n","Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard==2.9.0) (0.4.8)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard==2.9.0) (3.2.2)\n","Installing collected packages: tensorboard\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.9.1\n","    Uninstalling tensorboard-2.9.1:\n","      Successfully uninstalled tensorboard-2.9.1\n","Successfully installed tensorboard-2.9.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-lightning==1.6.0\n","  Downloading pytorch_lightning-1.6.0-py3-none-any.whl (582 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.1/582.1 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]!=2021.06.0,\u003e=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.0) (2022.11.0)\n","Requirement already satisfied: tensorboard\u003e=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.0) (2.9.0)\n","Requirement already satisfied: PyYAML\u003e=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.0) (6.0)\n","Requirement already satisfied: typing-extensions\u003e=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.0) (4.4.0)\n","Collecting torchmetrics\u003e=0.4.1\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.0) (4.64.1)\n","Requirement already satisfied: torch\u003e=1.8.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.0) (1.11.0+cu113)\n","Collecting pyDeprecate\u003c0.4.0,\u003e=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging\u003e=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.0) (21.3)\n","Requirement already satisfied: numpy\u003e=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.0) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (2.25.1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (3.8.3)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging\u003e=17.0-\u003epytorch-lightning==1.6.0) (3.0.9)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (2.16.0)\n","Requirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (0.6.1)\n","Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (1.3.0)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (1.0.1)\n","Requirement already satisfied: grpcio\u003e=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (1.51.1)\n","Requirement already satisfied: protobuf\u003e=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (3.19.6)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (3.4.1)\n","Requirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (0.38.4)\n","Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (57.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (0.4.6)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (1.3.1)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (1.3.3)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (1.8.2)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (22.2.0)\n","Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (4.0.2)\n","Requirement already satisfied: charset-normalizer\u003c3.0,\u003e=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (2.1.1)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (6.0.4)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (4.9)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (0.2.8)\n","Requirement already satisfied: six\u003e=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (1.15.0)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (5.2.1)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata\u003e=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (6.0.0)\n","Requirement already satisfied: chardet\u003c5,\u003e=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (4.0.0)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.8/dist-packages (from requests-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (2022.12.7)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests-\u003efsspec[http]!=2021.06.0,\u003e=2021.05.0-\u003epytorch-lightning==1.6.0) (1.24.3)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata\u003e=4.4-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (3.11.0)\n","Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (0.4.8)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard\u003e=2.2.0-\u003epytorch-lightning==1.6.0) (3.2.2)\n","Installing collected packages: pyDeprecate, torchmetrics, pytorch-lightning\n","Successfully installed pyDeprecate-0.3.2 pytorch-lightning-1.6.0 torchmetrics-0.11.0\n"]}],"source":["# Optional: install correct libraries in google colab\n","!python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n","!python -m pip install tensorboard==2.9.0\n","!python -m pip install pytorch-lightning==1.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K_X7wmcoq5MI"},"outputs":[],"source":["import sys\n","\n","# For google colab\n","!python -m pip install pytorch-lightning==1.6.0 \u003e /dev/null\n","\n","# For anaconda/regular python\n","# !{sys.executable} -m pip install pytorch-lightning==1.6.0 \u003e /dev/null\n","# 1. Preparation\n","\n","## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4873,"status":"ok","timestamp":1674613904397,"user":{"displayName":"Jay Chung","userId":"07183897499487652795"},"user_tz":-60},"id":"l00QbqpOvfiQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.8.0\n","  Downloading torchtext-0.8.0-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.8.0) (2.25.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.8.0) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.8.0) (1.21.6)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchtext==0.8.0) (1.11.0+cu113)\n","Requirement already satisfied: chardet\u003c5,\u003e=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etorchtext==0.8.0) (4.0.0)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etorchtext==0.8.0) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etorchtext==0.8.0) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests-\u003etorchtext==0.8.0) (2022.12.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch-\u003etorchtext==0.8.0) (4.4.0)\n","Installing collected packages: torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.14.1\n","    Uninstalling torchtext-0.14.1:\n","      Successfully uninstalled torchtext-0.14.1\n","Successfully installed torchtext-0.8.0\n"]}],"source":["!pip install -U torchtext==0.8.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"elapsed":236,"status":"error","timestamp":1674615041298,"user":{"displayName":"Jay Chung","userId":"07183897499487652795"},"user_tz":-60},"id":"QcJHqDMMq5MJ","outputId":"42782675-05c2-47e3-8098-025c4d430890"},"outputs":[{"ename":"OSError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-13-f1ec213dc780\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexercise_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexercise_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUtil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckParams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mexercise_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetworks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexercise_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_seg_nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#set up default cuda device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/i2dl/exercise_10/exercise_code/networks/segmentation_nn.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningDataModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLightningModule\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_stats_monitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceStatsMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/base.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_func\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmove_data_to_device\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAllGatherGrad\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m from pytorch_lightning.utilities.enums import (  # noqa: F401\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/apply_func.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compare_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TORCHTEXT_LEGACY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrank_zero_deprecation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/imports.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0m_TORCH_QUANTIZE_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_engines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meg\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0m_TORCHTEXT_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_package_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 116\u001b[0;31m \u001b[0m_TORCHTEXT_LEGACY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TORCHTEXT_AVAILABLE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_compare_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0.11.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0m_TORCHVISION_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_package_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0m_WANDB_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_package_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wandb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/imports.py\u001b[0m in \u001b[0;36m_compare_version\u001b[0;34m(package, op, version, use_base_version)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 69\u001b[0;31m         \u001b[0mpkg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# the following import has to happen first in order to load the torchtext C++ library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_extension\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0m_TEXT_BUCKET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://download.pytorch.org/models/text/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 64\u001b[0;31m \u001b[0m_init_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext C++ Extension is not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 58\u001b[0;31m     \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libtorchtext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;31m# This import is for initializing the methods registered via PyBind11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# This has to happen after the base library is loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 50\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkey_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 220\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;31m# Each OpOverload object contains pointer to a a specific operator overload, a pointer to the parent `OpOverloadPacket` object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;31m# You can obtain an OpOverload object through attribute query on OpOverloadPacket.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.8/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 373\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: /usr/local/lib/python3.8/dist-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN2at4_ops5zeros4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEE"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import torch\n","\n","from exercise_code.data.segmentation_dataset import SegmentationData, label_img_to_rgb\n","from exercise_code.data.download_utils import download_dataset\n","from exercise_code.util import visualizer, save_model\n","from exercise_code.util.Util import checkSize, checkParams, test\n","from exercise_code.networks.segmentation_nn import SegmentationNN, DummySegmentationModel\n","from exercise_code.tests import test_seg_nn\n","#set up default cuda device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"kKzA9lGAq5MJ"},"source":["## Setup TensorBoard\n","In exercise 07 you've already learned how to use TensorBoard. Let's use it again to make the debugging of our network and training process more convenient! Throughout this notebook, feel free to add further logs or visualizations to your TensorBoard!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6L1-EIJJq5MJ"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir lightning_logs --port 6006"]},{"cell_type":"markdown","metadata":{"id":"0_myJmbjq5MK"},"source":["## Load and Visualize Data\n","\n","#### MSRC-v2 Segmentation Dataset\n","\n","The MSRC v2 dataset is an extension of the MSRC v1 dataset from Microsoft Research in Cambridge. It contains *591* images and *23* object classes with accurate pixel-wise labeled images. \n","\n","\n","\n","The image ids are stored in the txt file `train.txt`, `val.txt`, `test.txt`. The dataloader will read the image id in the txt file and fetch the corresponding input and target images from the image folder. \n","\u003cimg src='images/input_target.png'/\u003e\n","\n","\n","\n","As you can see in `exercise_code/data/segmentation_dataset.py`, each segmentation label has its corresponding RGB value stored in the `SEG_LABELS_LIST`. The label `void` means `unlabeled`, and it is displayed as black `\"rgb_values\": [0, 0, 0]` in the target image. The target image pixels will be labeled based on its color using `SEG_LABELS_LIST`.\n","\n","```python\n","                SEG_LABELS_LIST = [\n","                {\"id\": -1, \"name\": \"void\",       \"rgb_values\": [0,   0,    0]},\n","                {\"id\": 0,  \"name\": \"building\",   \"rgb_values\": [128, 0,    0]},\n","                {\"id\": 1,  \"name\": \"grass\",      \"rgb_values\": [0,   128,  0]},\n","                {\"id\": 2,  \"name\": \"tree\",       \"rgb_values\": [128, 128,  0]},\n","                {\"id\": 3,  \"name\": \"cow\",        \"rgb_values\": [0,   0,    128]},\n","                {\"id\": 4,  \"name\": \"horse\",      \"rgb_values\": [128, 0,    128]},\n","                {\"id\": 5,  \"name\": \"sheep\",      \"rgb_values\": [0,   128,  128]},\n","                ...]    \n","```"]},{"cell_type":"markdown","metadata":{"id":"z-wTBVWBq5MK"},"source":["\u003cdiv class=\"alert alert-block alert-warning\"\u003e\n","    \u003ch3\u003eNote: The label \u003ccode\u003evoid\u003c/code\u003e\u003c/h3\u003e\n","    \u003cp\u003ePixels with the label \u003ccode\u003evoid\u003c/code\u003e should neither be considered in your loss nor in the accuracy of your segmentation. See implementation for details.\u003c/p\u003e\n","\u003c/div\u003e"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfEZesqeq5ML"},"outputs":[],"source":["download_url = 'https://i2dl.vc.in.tum.de/static/data/segmentation_data.zip'\n","i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n","data_root = os.path.join(i2dl_exercises_path, 'datasets','segmentation')\n","\n","\n","download_dataset(\n","    url=download_url,\n","    data_dir=data_root,\n","    dataset_zip_name='segmentation_data.zip',\n","    force_download=False,\n",")\n","\n","train_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/train.txt')\n","val_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/val.txt')\n","test_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/test.txt')"]},{"cell_type":"markdown","metadata":{"id":"KGTi6D6_q5ML"},"source":["If you want to implement data augmentation methods, make yourself familiar with the segmentation dataset and how we implemented the `SegmentationData` class in `exercise_code/data/segmentation_dataset.py`. Furthermore, you can check the original label description in `datasets/segmentation/segmentation_data/info.html`.\n","\n","For now, let's look at a few samples of our training set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zkv1R6YTq5ML"},"outputs":[],"source":["print(\"Train size: %i\" % len(train_data))\n","print(\"Validation size: %i\" % len(val_data))\n","print(\"Img size: \", train_data[0][0].size())\n","print(\"Segmentation size: \", train_data[0][1].size())\n","\n","num_example_imgs = 4\n","plt.figure(figsize=(10, 5 * num_example_imgs))\n","for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n","    # img\n","    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n","    plt.imshow(img.numpy().transpose(1,2,0))\n","    plt.axis('off')\n","    if i == 0:\n","        plt.title(\"Input image\")\n","    \n","    # target\n","    \n","    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n","    plt.imshow(label_img_to_rgb(target.numpy()))\n","    plt.axis('off')\n","    if i == 0:\n","        plt.title(\"Target image\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_WSbsg-Vq5MM"},"source":["We can already see that the dataset is quite small in comparison to our previous datasets, e.g., for CIFAR10 we had ten thousands of images while we only have 276 training images in this case. In addition, the task is much more difficult than a \"simple 10 class classification\", as we have to assign a label to each pixel! What's more, the images are much bigger as we are now considering images of size 240x240 instead of 32x32. \n","\n","That means that you shouldn't expect our networks to perform very well, so don't be too disappointed."]},{"cell_type":"markdown","metadata":{"id":"KvPUdaIRq5MM"},"source":["# 2. Semantic Segmentation \n","\n","## Dummy Model\n","\n","In `exercise_code/networks/segmentation_nn.py` we define a naive `DummySegmentationModel`, which always predicts the scores of segmentation labels of the first image. Let's try it on a few images and visualize the outputs using the `visualizer` we provide. The `visualizer` takes in the model and dataset, and visualizes the first four (Input, Target, Prediction) pairs. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5pzKymiq5MM"},"outputs":[],"source":["dummy_model = DummySegmentationModel(target_image=train_data[0][1])\n","\n","# Visualization function\n","visualizer(dummy_model, train_data)"]},{"cell_type":"markdown","metadata":{"id":"iKNyOShwq5MM"},"source":["You can use the visualizer function in your training scenario to print out your model predictions on a regular basis."]},{"cell_type":"markdown","metadata":{"id":"2S2P7RHcq5MM"},"source":["## Loss and Metrics\n","The loss function for the task of image segmentation is a pixel-wise cross entropy loss. This loss examines each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPsQz5lGyhMM"},"outputs":[],"source":["from PIL import Image\n","\n","Image.open('/content/gdrive/MyDrive/i2dl/exercise_10/images/loss_img.png')"]},{"cell_type":"markdown","metadata":{"id":"r3CLXZNoyfUK"},"source":["\n","https://www.jeremyjordan.me/semantic-segmentation/\n","\n","Up until now we only used the default loss function (`nn.CrossEntropyLoss`) in our solvers. However, In order to ignore the `unlabeled` pixels for the computation of our loss, we have to use a customized version of the loss for the initialization of our segmentation solver. The `ignore_index` argument of the loss can be used to filter the `unlabeled` pixels and computes the loss only over remaining pixels.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYDirJZ8q5MM"},"outputs":[],"source":["loss_func = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n","\n","for (inputs, targets) in train_data[0:4]:\n","    inputs, targets = inputs, targets\n","    outputs = dummy_model(inputs.unsqueeze(0))\n","    losses = loss_func(outputs, targets.unsqueeze(0))\n","    print(losses)"]},{"cell_type":"markdown","metadata":{"id":"btOnKtxzq5MM"},"source":["\u003cdiv class=\"alert alert-warning\"\u003e\n","    \u003ch3\u003eNote: Non-zero loss for the first sample\u003c/h3\u003e \n","    \u003cp\u003eThe output of our dummy model is one-hot-coded tensor. Since there is \u003cb\u003esoftmax\u003c/b\u003e function in the \u003cb\u003enn.CrossEntropyLoss\u003c/b\u003e function, the loss is:\u003c/p\u003e\n","\u003c/div\u003e\n","\n","$$loss(x, class) = - \\log \\left( \\frac{\\exp(x[class])}{\\Sigma_j \\exp (x[j])} \\right) = −x[class]+\\log \\left( \\Sigma_j \\exp(x[j]) \\right)$$ \n","\n","and the loss will not be zero.\n","\n","i.e. for $x=[0, 0, 0, 1], class=3$\n","\n","the loss:\n","\n","$$loss(x,class) = -1 +\\log(\\exp(0)+\\exp(0)+\\exp(0)+\\exp(1)) = 0.7437$$"]},{"cell_type":"markdown","metadata":{"id":"k3f4PVIOq5MN"},"source":["To obtain an evaluation accuracy, we can simply compute the average per pixel accuracy of our network for a given image. We will use the following function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mML_QPMq5MN"},"outputs":[],"source":["def evaluate_model(model, dataloader):\n","    test_scores = []\n","    model.eval()\n","    for inputs, targets in dataloader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        outputs = model.forward(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        targets_mask = targets \u003e= 0\n","        test_scores.append(np.mean((preds.cpu() == targets.cpu())[targets_mask].numpy()))\n","\n","    return np.mean(test_scores)\n","\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=1,shuffle=False,num_workers=0)\n","# print(evaluate_model(dummy_model, test_loader))"]},{"cell_type":"markdown","metadata":{"id":"20RenkHYq5MN"},"source":["You will see reasonably high numbers as your accuracy when you do the training later. The reason behind that is the fact that most output pixels are of a single class and the network can just overfit to common classes such as \"grass\"."]},{"cell_type":"markdown","metadata":{"id":"_AOjNTzwq5MN"},"source":["## Step 1: Design your own model\n","\n","\u003cdiv class=\"alert alert-info\"\u003e\n","    \u003ch3\u003eTask: Implement\u003c/h3\u003e\n","    \u003cp\u003eImplement your network architecture in \u003ccode\u003eexercise_code/networks/segmentation_nn.py\u003c/code\u003e. In this task, you will use pytorch to setup your model.\n","    \u003c/p\u003e\n","\u003c/div\u003e\n","\n","To compensate for the dimension reduction of a typical convolution layer, you should probably include either a single `nn.Upsample` layer, use a combination of upsampling layers as well as convolutions or even transposed convolutions near the end of your network to get back to the target image shape.\n","\n","This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically). \n","The only rules your model design has to follow are:\n","* Inherit from `torch.nn.Module` or `pytorch_lightning.LightningModule`\n","* Perform the forward pass in `forward()`. Input dimension is (N, C, H, W) and output dimension is (N, num_classes, H, W)\n","* Have less than 5 million parameters\n","* Have a model size of less than 50MB after saving\n","\n","Furthermore, you need to pass all your hyperparameters to the model in a single dict `hparams`."]},{"cell_type":"markdown","metadata":{"id":"GxTAqUOoq5MN"},"source":["\u003cdiv class=\"alert alert-warning\"\u003e\n","    \u003ch3\u003eNote: Transfer learning\u003c/h3\u003e\n","    \u003cp\u003eIn this exercise, we encourage you to do transfer learning as we learned in exercise 8, since this will boost your model performance and save training time. You can import pretrained models from torchvision in your model and use its feature extractor (e.g. \u003ccode\u003ealexnet.features\u003c/code\u003e) to get the image feature. Feel free to choose more advanced pretrained model like ResNet, MobileNet for your architecture design.\u003c/p\u003e       \n","\u003c/div\u003e\n","\n","See [here](https://pytorch.org/vision/stable/models.html) for more info of the torchvison pretrained models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVeLf9orq5MN"},"outputs":[],"source":["hparams = {\n","    \"batch_size\" : 47,\n","    \"epochs\" : 100,\n","    \n","    # optimizer\n","    \"learning_rate\" : 3e-4,\n","    \"weight_decay\": 1e-5,\n","    'momentum': 0.9\n","    # TODO: if you have any model arguments/hparams, define them here and read them from this dict inside SegmentationNN class\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1_q8Lhdq5MN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7063WW6XSYZ"},"outputs":[],"source":["# https://medium.com/the-owl/extracting-features-from-an-intermediate-layer-of-a-pretrained-model-in-pytorch-c00589bda32b\n","# https://colab.research.google.com/github/usuyama/pytorch-unet/blob/master/pytorch_unet_resnet18_colab.ipynb#scrollTo=CRIOwoQvBKPm\n","# https://stackoverflow.com/questions/52235520/how-to-use-pnasnet5-as-encoder-in-unet-in-pytorch\n","# import torchvision.models as models\n","\n","# rn18 = models.resnet18(pretrained=True)\n","# children_counter = 0\n","# for n,c in rn18.named_children():\n","#     print(\"Children Counter: \",children_counter,\" Layer Name: \",n,)\n","#     children_counter+=1\n","# rn18._modules"]},{"cell_type":"markdown","metadata":{"id":"LqR6h-4BhfDS"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzJ5RaT8mRAS"},"outputs":[],"source":["# from exercise_code.networks.segmentation_nn import DummySegmentationModel, DoubleConv, InConv, Down, Up, OutConv, Unet\n","from exercise_code.networks.segmentation_nn import SegmentationNN\n","\n","model = SegmentationNN()\n","test_seg_nn(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmDBdwXL_c1Q"},"outputs":[],"source":["import torchvision.models as models\n","alexnet = models.alexnet(pretrained=True).features\n","alexnet"]},{"cell_type":"markdown","metadata":{"id":"xh3fGth1q5MN"},"source":["## Step 2: Train your own model\n","\n","\u003cdiv class=\"alert alert-info\"\u003e\n","    \u003ch3\u003eTask: Implement\u003c/h3\u003e\n","    \u003cp\u003e In addition to the network itself, you will also need to write the code for the model training. You can use PyTorch Lightning for that, or you can also write it yourself in standard PyTorch.\n","    \u003c/p\u003e\n","\u003c/div\u003e"]},{"cell_type":"markdown","metadata":{"id":"PdY3OHrDQTZM"},"source":["--- 랜덤서치\n","https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n","\n","-- 러닝레이트 플라토 \n","https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nm395GhJq5MO"},"outputs":[],"source":["from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import pytorch_lightning as pl\n","from torchvision import transforms\n","import torch.optim as optim \n","from torch.utils.data import DataLoader\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","\n","\n","########################################################################\n","# TODO - Train Your Model                                              #\n","########################################################################\n","\n","# dataset \n","transfomr = transforms.Compose(transforms.ToTensor())\n","\n","train_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/train.txt')\n","val_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/val.txt')\n","test_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/test.txt')\n","\n","# data loader \n","train_loader = torch.utils.data.DataLoader(train_data,\n","                                           batch_size=hparams['batch_size'],\n","                                           shuffle=False,\n","                                           num_workers=2)\n","\n","val_loader = torch.utils.data.DataLoader(val_data, \n","                                         batch_size=hparams['batch_size'],\n","                                         shuffle=False, \n","                                         num_workers=2)\n"," \n","# moveing the model to the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model = model.to(device)\n","\n","# optimizer and loss function\n","optimizer = optim.Adam(model.parameters(),\n","                       lr = hparams['learning_rate'],\n","                       weight_decay = hparams['weight_decay'])\n","\n","loss_func = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n","\n","# epoch \n","epochs = hparams['epochs']\n","\n","# initializing thel ist for storing the loss and accuracy\n","train_loss_history = []\n","train_acc_history = []\n","\n","# get the number of batches \n","def get_num_batches(dataloader, data):\n","  return int(len(batch) // dataloader.batch_size)\n","\n","# TODO : Learning Scheduler : https://velog.io/@idj7183/Competition%EC%97%90%EC%84%9C-%EC%83%88%EB%A1%9C-%EB%B0%B0%EC%9A%B4-%EA%B2%83\n","def get_scheduler(optimizer, step_size, gamma):\n","  return torch.optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)\n","\n","# create progress bar \n","def create_tqdm_bar(iterable, desc):\n","  return tqdm(enumerate(iterable), total=len(iterable), ncols = 150, desc = desc, leave = False)\n","\n","# Training \n","def train_model(model, train_loader, val_loader, loss_func, optimizer, writer, epochs = 50):\n","  optimizer = optimizer\n","  # scheduler = get_scheduler(optimizer = optimizer, step_size = epochs * len(train_loader) / 20, gamma = 0.6)\n","  # scheduler = get_scheduler(optimizer = optimizer, step_size = 50, gamma = 0.7)\n","  scheduler = ReduceLROnPlateau(optimizer, 'min', factor = 0.95, min_lr = 1e-5)\n","  # scheduler = CosineAnnealingWarmRestarts(optimizer, \n","                                        # T_0 = 8,# Number of iterations for the first restart\n","                                        # T_mult = 1, # A factor increases TiTi​ after a restart\n","                                        # eta_min = 1e-4) # Minimum learning rate\n","\n","  for epoch in range(epochs):\n","    # train \n","    model.train()\n","    train_running_loss = []  # 출력용\n","    for train_iteration, train_batch in enumerate(train_loader):\n","      input, target = train_batch \n","      input, target = input.to(device), target.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      preds = model(input)\n","      loss = loss_func(preds, target)\n","      train_running_loss.append(loss.item())\n","\n","      loss.backward()\n","      optimizer.step()\n","\n","      # update the tensorboarrd writer ( train )\n","      writer.add_scalar('train_loss', train_learning_loss, epoch * len(train_loader) + train_iteration)\n","\n","    # update step based scheduler\n","    # scheduler.step()\n","\n","    # validation \n","    model.eval()\n","    validation_running_loss = [] # 출력용\n","    with torch.no_grad():\n","      # for val_iteration, val_batch in validation_loop:\n","      for val_iteration, val_batch in enumerate(val_loader):\n","        input, target = val_batch \n","        input, target = input.to(device), target.to(device) \n","\n","        preds = model(input)\n","        loss = loss_func(preds, target)\n","\n","        # learning rate step when plateau \u003c- Plateau Scheduler\n","        scheduler.step(loss) \n","\n","        # validation_loss += loss.item()\n","        validation_running_loss.append(loss.item())\n","\n","        # update the tensorboard writer ( val )\n","        writer.add_scalar('val_loss', loss.item(), epoch * len(val_loader) + val_iteration)\n","\n","    # print current lr\n","    print('current learning rate : ',  scheduler._last_lr)\n","      \n","    # print current validation loss\n","    print(\"EPOCH %04d / %04d | %12s : %.6f | %12s : %0.6f\"  % (epoch + 1, epochs, 'VALIDATION LOSS', np.mean(validation_running_loss), 'TRAIN LOS', np.mean(train_running_loss)))\n","\n","\n","# Tensorboard, summarywriter \n","from torch.utils.tensorboard import SummaryWriter\n","base_dir = '/content/gdrive/MyDrive/i2dl/exercise_10'\n","log_dir = os.path.join(base_dir, 'log')\n","writer = SummaryWriter(log_dir = log_dir)\n","\n","# train \n","epochs = hparams['epochs']\n","train_model(model, train_loader, val_loader, loss_func, optimizer, writer, epochs)\n","\n","# save the loss on tensorboard \n","writer.close()\n","\n","#######################################################################\n","#                           END OF YOUR CODE                          #\n","#######################################################################\n","\n","# https://medium.com/analytics-vidhya/creating-a-very-simple-u-net-model-with-pytorch-for-semantic-segmentation-of-satellite-images-223aa216e705\n","# https://medium.com/the-owl/extracting-features-from-an-intermediate-layer-of-a-pretrained-model-in-pytorch-c00589bda32b\n","# https://dacon.io/codeshare/4245\n","# https://dacon.io/codeshare/4245"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XB2pp75pbb98"},"outputs":[],"source":["tensorboard --logdir ./logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3R3S5A-q5MO"},"outputs":[],"source":["model = model.to(device)\n","test(evaluate_model(model, test_loader))"]},{"cell_type":"markdown","metadata":{"id":"fbouqP8Wq5MO"},"source":["# 4. Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Z7OC89Kq5MO"},"outputs":[],"source":["visualizer(model, test_data)"]},{"cell_type":"markdown","metadata":{"id":"-oEIVvc6q5MO"},"source":["## Save the Model for Submission\n","\n","When you are satisfied with your training, save the model for [submission](https://i2dl.vc.in.tum.de/submission/). In order to be eligible for the bonus points you have to achieve an accuracy above __64%__."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFCfJd8Dq5MO"},"outputs":[],"source":["os.makedirs('models', exist_ok=True)\n","save_model(model, \"segmentation_nn.model\")\n","checkSize(path = \"./models/segmentation_nn.model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LkDkTW0q5MO"},"outputs":[],"source":["from exercise_code.util.submit import submit_exercise\n","\n","submit_exercise('../output/exercise10')"]},{"cell_type":"markdown","metadata":{"id":"tZBXhAl-q5MO"},"source":["# Submission Instructions\n","\n","Congratulations! You've just built your first semantic segmentation model with PyTorch Lightning! To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n","\n","1. Go on [our submission page](https://i2dl.vc.in.tum.de/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an ID which we need in the next step.\n","2. Log into [our submission page](https://i2dl.vc.in.tum.de/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted file selectable on the top.\n","3. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold.\n","\n","# Submission Goals\n","\n","- Goal: Implement and train a convolutional neural network for Semantic Segmentation.\n","- Passing Criteria: Reach **Accuracy \u003e= 64%** on __our__ test dataset. The submission system will show you your score after you submit.\n","- Submission start: __January 19, 2023 - 13:00__\n","- Submission deadline: __January 25, 2023 - 15:59__\n","- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"]},{"cell_type":"markdown","metadata":{"id":"fmYGZWZNq5MP"},"source":["# [Exercise Review](https://docs.google.com/forms/d/e/1FAIpQLScwZArz6ogLqBEj--ItB6unKcv0u9gWLj8bspeiATrDnFH9hA/viewform)\n","\n","We are always interested in your opinion. Now that you have finished this exercise, we would like you to give us some feedback about the time required to finish the submission and/or work through the notebooks. Please take the short time to fill out our [review form](https://docs.google.com/forms/d/e/1FAIpQLScwZArz6ogLqBEj--ItB6unKcv0u9gWLj8bspeiATrDnFH9hA/viewform) for this exercise so that we can do better next time! :)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIY3p9k_YXxb"},"outputs":[],"source":["\n","################\n","# def fit(model, train_data, train_loader, loss_func, optimizer, writer_train, epoch):\n","#   model.train()\n","\n","#   # for epoch in range(epochs):\n","#   train_running_loss = 0.0\n","#   count = 0\n","\n","#   # get the number of batches for the tqdm progress bar \n","#   num_batches = get_num_batches(train_loader, train_data)\n","\n","#   for i, train_batch in enumerate(train_loader):\n","#     count += 1\n","\n","#     input, target = train_batch\n","#     input, target = input.to(device), target.to(device)\n","\n","#     optimizer.zero_grad()\n","\n","#     preds = model(input)\n","#     loss = loss_func(preds, target)\n","#     train_running_loss += loss.item()\n","\n","#     loss.backward() # ? \n","#     optimizer.step()\n","\n","#     # update the tensorboard \n","#     writer_train.add_scaler('train_loss', loss, epoch * len(train_loader) + i)\n","  \n","#   return train_running_loss / count\n","\n","\n","\n","# def eval(model, val_loader, loss_func, optimizer):  \n","#   model.eval()\n","\n","#   # for epoch in range(epochs):\n","#   val_running_loss = 0.0\n","#   count = 0\n","  \n","#   with torch.no_grad():\n","#     for i, val_batch in enumerate(val_loader):\n","#       count += 1\n","\n","#       input, target = val_batch \n","#       input, target = input.to(device), target.to(device)\n","\n","#       preds = model(input)\n","#       loss = loss_func(preds, target)\n","#       val_running_loss += loss.item()\n","\n","#       # update the tensorboard \n","#       writer_train.add_scaler('val_loss', loss, epoch * len(val_loader) + i)\n","\n","#   return val_running_loss / count \n","  \n","\n","\n","\n","# # Tensorboard, summarywriter \n","# from torch.utils.tensorboard import SummaryWriter\n","# base_dir = '/content/gdrive/MyDrive/i2dl/exercise_10'\n","# log_dir = os.path.join(base_dir, 'log')\n","# writer_train = SummaryWriter(log_dir = os.path.join(log_dir, 'train'))\n","# writer_val = SummaryWriter(log_dir = os.path.join(log_dir, 'val'))\n","\n","# # Import progressbar \n","# from tqdm import tqdm\n","\n","# ## train the model\n","# train_loss = [] \n","# val_loss = []\n","# for epoch in range(epochs):\n","#   train_epoch_loss = fit(model, train_loader, loss_func, optimizer, writer_train, epoch)\n","#   val_epoch_loss = eval(model, val_loader, loss_func, optimizer, writer_val, epoch)\n","#   train_loss.append(train_epoch_loss)\n","#   val_loss.append(val_epoch_loss)\n","#   print(f'Epoch : {epoch + 1} / {epochs} :: Train Loss: {train_epoch_loss:.8f} / Validation Loss: {val_epoch_loss:.8f} ')\n","\n","\n","    \n","\n","\n","#\n","# trainer = pl.Trainer( max_epochs = 50,\n","#                       accelerator=\"gpu\", devices=-1)\n","\n","\n","# train_dataloader = DataLoader(train_data, batch_size = hparams[\"batch_size\"])\n","# val_dataloader = DataLoader(val_data, batch_size = hparams[\"batch_size\"])\n","\n","# trainer.fit(model, train_dataloader, val_dataloader)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXHAsAYoLwGv"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}